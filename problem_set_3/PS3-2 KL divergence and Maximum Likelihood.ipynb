{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PS3-2 KL divergence and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (a) Nonnegativity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For any $P$, $Q$,\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL} (P \\Vert Q) & = H(P, Q) - H(P) \\\\\n",
    "                   & = - \\sum_{x \\in \\mathcal{X}} P(x) \\log Q(x) - \\big( - \\sum_{x \\in \\mathcal{X}} P(x) \\log P(x) \\big) \\\\\n",
    "                   & = - \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{Q(x)}{P(x)} \\\\\n",
    "                   & \\geq - \\log \\sum_{x \\in \\mathcal{X}} P(x) \\frac{Q(x)}{P(x)} \\\\\n",
    "                   & = - \\log \\sum_{x \\in \\mathcal{X}} Q(x) \\\\\n",
    "                   & = 0\n",
    "\\end{align*}\n",
    "\n",
    "By Jensen's Inequality, the equality of the forth step holds if and only if $Q(x) = cP(x)$ for some $c$.\n",
    "We have equality in the last step $\\sum_{x \\in \\mathcal{X}} Q(x) = \\sum_{x \\in \\mathcal{X}} P(x) = 1$, which implies $c = 1$.\n",
    "Therefore, $D_{KL} (P \\Vert Q) = 0$ if and only if $P = Q$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (b) Chain rule for KL divergence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align*}\n",
    "D_{KL} (P(X, Y) \\Vert Q(X, Y)) & = H(P(X, Y), Q(X, Y)) - H(P(X, Y)) \\\\\n",
    "                               & = - \\sum_x \\sum_y P(x, y) \\log Q(x, y) - \\big( - \\sum_x \\sum_y P(x, y) \\log P(x, y) \\big) \\\\\n",
    "                               & = - \\sum_x \\sum_y P(y \\ \\vert \\ x) P(x) \\log Q(y \\ \\vert \\ x) Q(x) + \\sum_x \\sum_y P(y \\ \\vert \\ x) P(x) \\log P(y \\ \\vert \\ x) P(x) \\\\\n",
    "                               & = - \\sum_x \\sum_y P(y \\ \\vert \\ x) P(x) \\log Q(y \\ \\vert \\ x) - \\sum_x \\sum_y P(y \\ \\vert \\ x) P(x) \\log Q(x) + \\sum_x \\sum_y P(y \\ \\vert \\ x) P(x) \\log P(y \\ \\vert \\ x) + \\sum_x \\sum_y P(y \\ \\vert \\ x) P(x) \\log P(x) \\\\\n",
    "                               & = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\sum_y P(y \\ \\vert \\ x) + \\sum_x P(x) \\sum_y P(y \\ \\vert \\ x) \\log \\frac{P(y \\ \\vert \\ x)}{Q(y \\ \\vert \\ x)} \\\\\n",
    "                               & = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\cdot 1 + \\sum_x P(x) \\big( \\sum_y P(y \\ \\vert \\ x) \\log \\frac{P(y \\ \\vert \\ x)}{Q(y \\ \\vert \\ x)} \\big) \\\\\n",
    "                               & = D_{KL} (P(X) \\Vert Q(X)) + D_{KL} (P(Y \\ \\vert X) \\Vert Q(Y \\ \\vert \\ X))\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (c) KL and maximum likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the empirical distribution\n",
    "\n",
    "$$\\hat{P}(x) = \\frac{1}{m} \\sum_{i = 1}^{m} 1 \\{ x^{(i)} = x \\}$$\n",
    "\n",
    "$\\mathbb{E}[f]$ can be approximated as\n",
    "\n",
    "$$\\mathbb{E}[f] \\approx \\frac{1}{m} \\sum_{i = 1}^{m} f(x^{(i)})$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg \\min_\\theta D_{KL}(\\hat{P} \\Vert P_\\theta) & = \\arg \\min_\\theta H(\\hat{P}, P_\\theta) - H(\\hat{P}) \\\\\n",
    "                                                & = \\arg \\min_\\theta - \\sum_x \\hat{P}(x) \\log P_\\theta (x) + \\sum_x \\hat{P}(x) \\log \\hat{P} (x) \\\\\n",
    "                                                & \\approx \\arg \\min_\\theta \\frac{1}{m} \\sum_{i = 1}^{m} \\big( - \\log P_\\theta (x^{(i)}) + \\hat{P}(x^{(i)}) \\big) \\\\\n",
    "                                                & = \\arg \\max_\\theta \\sum_{i = 1}^{m} \\log P_\\theta (x^{(i)})\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cs229",
   "language": "python",
   "display_name": "cs229"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}